////
NO CAMBIAR!!
Codificación, idioma, tabla de contenidos, tipo de documento
////
:encoding: utf-8
:lang: es
:toc: right
:toc-title: Tabla de contenidos
:doctype: book
:imagesdir: ./images


////
Nombre y título del trabajo
////
# Instalación de OpenStack
Cloud-DI
Cloud-DI Team <clouddi@ual.es>

image::di.png[]

// NO CAMBIAR!! (Entrar en modo no numerado de apartados)
:numbered!: 


[abstract]
## Resumen

OpenStack es un software para la creación de clouds. El proyecto OpenStack ofrece gran cantidad de componentes para implementar los diferentes servicios que se quieran desplegar en nuestro cloud (máquinas virtuales, infraestructura de red, almacenamiento de bloques, almacenamiento de archivos compartidos, almacenamiento de objetos, orquestación, monitorización, por citar algunos). OpenStack tiene una arquitectura totalmente modular en la que partiendo de los componentes básicos (_Keystone_ para autenticación, _Glance_ para imágenes, _Neutron_ para redes y _Nova_ para instancias) podemos ir añadiendo los componentes adecuados para poder desplegar los servicios que queremos ofrecer.

En este tutorial se describen los pasos de instalación de los componentes que actualmente están disponibles en https://openstack.di.ual.es/horizon[OpenStack-DI].

[IMPORTANT]
====
El acceso a OpenStack-DI y a otros recursos más de Cloud-DI se puede hacer directamente desde https://openstack.di.ual.es/horizon[https://openstack.di.ual.es/horizon] o desde la web de servicios de Cloud-DI https://cloud.di.ual.es[https://cloud.di.ual.es]. Allí además hay disponible una lista de respuestas a preguntas frecuentes y los https://cloud.di.ual.es/TerminosServicio.html[Términos de servicio de Cloud-DI].

En https://moodle.di.ual.es[https://moodle.di.ual.es] se encuentran disponibles una serie de videos ilustrativos sobre los sevicios ofrecidos por Cloud-DI y sobre las operaciones básicas para el uso de OpenStack-DI.

Para realizar operaciones repetitivas o avanzadas es conveniente utilizar el CLI. Aquí está disponible la https://docs.openstack.org/python-openstackclient/pike/cli/command-list.html#command-list[Lista de comandos CLI].
====

// Entrar en modo numerado de apartados
:numbered:

//// 
COLOCA A CONTINUACION EL TITULO DEL APARTADO
////

## Componentes de OpenStack

OpenStack es un proyecto formado por distintos componentes. Cada uno de ellos añade una función a nuestro cloud (networking, almacenamiento, máquinas virtuales, orquestación, ...). Podemos clasificar estos componentes en básicos y opcionales.

* Componentes básicos 
** _Keystone_: Servicio de Identidad (*)
** _Neutron_: Servicio de redes (*)
** _Glance_: Servicio de imágenes (*)
** _Nova_: Servicio de cómputo (máquinas virtuales) (*)

* Componentes opcionales
** _Horizon_: Interfaz web (*)
** _Cinder_: Almacenamiento de bloques (*)
** _Manila_: Servicio de sistemas de archivos compartidos (*)
** _Swift_: Servicio de almacenamiento de objetos (*)
** _Sahara_: Servicio de procesamiento de datos (*)
** _Ceilometer_: Servicio de recolección de datos de telemetría
** _Aodh_: Servicio de alerta de telemetría
** _Heat_: Servicio de orquestación (*)
** _Barbican_: Servicio de gestión de claves (*)
** _Ironic_: Aprovisionamiento en máquinas físicas
** _Cloudkitty_: Servicio de tarificación

[NOTE]
====
Los componentes marcardos con asterisco (*) están disponibles actualmente en https://openstack.di.ual.es/horizon[OpenStack-DI].
====

La figura siguiente ilustra los componentes instalados en OpenStack-DI y su interacción básica. Los componentes en rojo son los componentes básicos.

.Componentes de OpenStack-DI
image::ComponentesOpenStack.png[]

* _Keystone_ proporciona servicios de identificación a todos los componentes OpenStack
* _Horizon_ proporciona un portal web de acceso al resto de componentes salvo a _Keystone_.
* _Glance_ proporciona las imágenes al crear las máquinas virtuales.
* _Neutron_ proporciona los servicios de networking a las máquinas virtuales
* _Cinder_ propociona almacenamiento de bloques a las máquinas virtuales. _Cinder_ también puede guardar snapshots de volumen.
* _Manila_ propociona servicios de almacenamiento compartido de archivos a las máquinas virtuales
* _Swift_ propociona almacenamiento de objetos a las máquinas virtuales y a _Sahara_. Opcionalmente se puede configurar _Nova_ para almacenar las imágenes en _Swift_.
* _Ceilometer_ recoge medidas de uso de los componentes de networking, imágenes, cómputo, almacenamiento y procesamiento de datos.
* _Heat_ permite la creación de stacks para la creación de infraestructura mediante código. Opcionalmente se puede combinar con _Ceilometer_ par ajustar dinánicamente la infraestructura en función del uso de recursos (RAM, cores, almacenamiento) recopilado por _Ceilometer_.

## Preparación del entorno

Para la instalación de los componentes de este tutorial partimos del siguiente escenario en el que contaremos con servidores dedicados para Control, Red y Cómputo. Los servicios de almacenamiento tienen los requisitos siguientes:

* _Cinder_: Almacenamiento en un NAS Synology y servicios ejecutándose en el nodo de Control.
* _Manila_: Servidor independiente.
* _Swift_: Dos servidores para proporcionar tolerancia a fallos.

La figura siguiente ilustra la arquitectura de referencia que usaremos en este tutorial. Tal y como aparece en la https://docs.openstack.org/ocata/install-guide-ubuntu/environment-networking.html[Guía de networking en la instalación de OpenStack] dispondremos de una red de mantenimiento, una red de túnel y la red externa. 

.Configuración y conexión de servidores
image::configuracionDeseable.png[]

Como se observa en la figura, todos los servidores están conectados a las redes de mantenimiento y túnel. Además, los servidores siguientes están contectados al exterior:

* Control: Proporciona acceso a la consola de _Horizon_ en la red de la UAL.
* Red: Ofrece conectividad a la red de la UAL a las máquinas virtuales.
* Almacenamiento compartido: Permite ofrecer sistemas de archivos de compartidos en la red de la UAL.

Los requisitos hardware mínimos de cada servidor son los que aparecen el la https://docs.openstack.org/ocata/install-guide-ubuntu/overview.html#example-architecture[arquitectura de ejemplo de la guía de instalación de OpenStack].

### Configuración de las interfaces de red

Es recomendable, aunque no necesario, una nomenclatura uniforme de las interfaces de red de los servidores que ofrecen la infraestructura a OpenStack. Si hay diferencias, recomendamos seguir la denominación clásica `eth0`, `eth1`, ... Sigue como `root` estos pasos cambiar los nombres de la interfaces de red a `eth0`, `eth1`, ...

1. Editar `/etc/default/grub` y cambiar la línea `GRUB_CMDLINE_LINUX=""` por  `GRUB_CMDLINE_LINUX="net.ifnames=0 biosdevname=0"`.
2. Actualizar GRUB con `update-grub`.
3. Actualizar el archivo `/etc/network/interfaces` con las interfaces de red ya a `eth0`, `eth1`, ...
4. Reiniciar el sistema con `reboot`

## Preparación de las máquinas

. En cada máquina crear un archivo `/etc/hosts` con las direcciones IP de la red de mantenimiento y los nombres que vayamos a dar a las máquinas:

+
[source, bash]
----
10.0.0.51 testcontroller

10.0.0.52 testnetwork

10.0.0.53 testcompute01
10.0.0.54 testcompute02
10.0.0.55 testcompute03
10.0.0.56 testcompute04

10.0.0.61 testobject01
10.0.0.62 testobject02

10.0.0.63 testshared
----
+

. Instalar `chrony` en todas las máquinas

+
[source, bash]
----
# apt-get install chrony
----
+

. Modificar en la máquina de control el archivo `/etc/chrony/chrony.conf`

+
.Archivo `/etc/chrony/chrony.conf` en el nodo de control
****
[source, bash]
----
pool 2.debian.pool.ntp.org offline iburst

server 1.es.pool.ntp.org iburst <1>
allow 10.0.0.0/24 <2>

keyfile /etc/chrony/chrony.keys

commandkey 1

driftfile /var/lib/chrony/chrony.drift

log tracking measurements statistics
logdir /var/log/chrony

maxupdateskew 100.0

dumponexit

dumpdir /var/lib/chrony

logchange 0.5

hwclockfile /etc/adjtime

rtcsync
----
<1> Servidor NTP
<2> Red de mantenimiento
****
+

. Modificar en el resto de máquinas el archivo `/etc/chrony/chrony.conf`

+
.Archivo `/etc/chrony/chrony.conf` en el resto de nodos
****
---
[source, bash]
----
server {{ nodes.controller.name }} iburst <1>

keyfile /etc/chrony/chrony.keys

commandkey 1

driftfile /var/lib/chrony/chrony.drift

log tracking measurements statistics
logdir /var/log/chrony

maxupdateskew 100.0

dumponexit

dumpdir /var/lib/chrony

logchange 0.5

hwclockfile /etc/adjtime

rtcsync
----
<1> Nombre del servidor de control
****
+

. Reiniciar `chrony` en todos los nodos

+
[source, bash]
----
# service chrony restart
----
+

. Añadir el repositorio de OpenStack Ocata en todos los nodos

+
[source, bash]
----
# apt-get install software-properties-common
# add-apt-repository cloud-archive:ocata
# apt update && apt dist-upgrade
----
+

. Instalar el cliente Python para OpenStack en todos los nodos

+
[source, bash]
----
# apt install python-openstackclient
----
+

. Instalar la base de datos en el nodo de control

+
[source, bash]
----
# apt-get install mariadb-server python-pymysql libmysqlclient-dev
----

. Modificar el archivo `/etc/mysql/mariadb.conf.d/99-openstack.cnf` en el nodo de control

+
.Archivo `/etc/mysql/mariadb.conf.d/99-openstack.cnf`
****
[source, bash]
----
[mysqld]
bind-address = {{ nodes.controller.management_ip }} <1>

default-storage-engine = innodb
innodb_file_per_table = on
max_connections = 4096
collation-server = utf8_general_ci
character-set-server = utf8
----
<1> Dirección IP de mantenimiento del nodo de control
****

. Modificar el archivo `/root/my.cnf` en el nodo de control

+
.Archivo `/root/my.cnf`
****
[source, bash]
----
[client]
user=root
password={{ mysql_root_password }} <1>
----
<1> Contraseña del usuario `root` de MySQL
****

+
[source, bash]
----
# service mysql restart
# mysql_secure_installation
----


. Instalar la cola de mensajes en el nodo de control

+
[source, bash]
----
# apt install rabbitmq-server
# rabbitmqctl add_user openstack {{ RABBIT_PASS }} <1>
# rabbitmqctl set_permissions openstack ".*" ".*" ".*"
----
<1> Contraseña de RabbitMQ

. Instalar Memcached en el nodo de control

+
[source, bash]
----
# apt install memcached python-memcache
----

+

. Modificar el archivo `/etc/memcached.conf`

+

.Archivo `/etc/memcached.conf`
****
[source, bash]
----
-d

logfile /var/log/memcached.log

-m 64

-p 11211

-u memcache

-l {{ nodes.controller.management_ip }} <1>
----
<1> Dirección IP de mantenimiento del nodo de control
****

. Reiniciar Memcached

+
[source, bash]
----
# service memcached restart
----

## Instalación de Keystone

La instalación de Keystone se realiza en el nodo de control

. Creación y configuración de la base de datos `keystone`

+
[source, bash]
----
MariaDB [(none)]> CREATE DATABASE keystone;
Grant proper access to the keystone database:

MariaDB [(none)]> GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \
IDENTIFIED BY {{ 'KEYSTONE_DBPASS' }}; <1>
MariaDB [(none)]> GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \
IDENTIFIED BY {{ 'KEYSTONE_DBPASS' }}; <2>
----
<1> Contraseña del usuario Keystone
<2> Contraseña del usuario Keystone

. Instalar los paquetes de Keystone 


+
[source, bash]
----
# apt install keystone
----

. Configurar el archivo `/etc/keystone.conf`

+
.El archivo `/etc/keystone.conf`
****
[source, bash]
----
[DEFAULT]

[assignment]

[auth]

[cache]

[catalog]

[cors]

[cors.subdomain]

[credential]

[database]

connection = mysql+pymysql://keystone:{{ keystone_dbpass }}@{{ nodes.controller.name }}/keystone <1>

[domain_config]

[endpoint_filter]

[endpoint_policy]

[eventlet_server]

[extra_headers]

[federation]

[fernet_tokens]

[healthcheck]

[identity]

[identity_mapping]

[kvs]

[ldap]

[matchmaker_redis]

[memcache]

[oauth1]

[oslo_messaging_amqp]

[oslo_messaging_kafka]

[oslo_messaging_notifications]

[oslo_messaging_rabbit]

[oslo_messaging_zmq]

[oslo_middleware]

[oslo_policy]

[paste_deploy]

[policy]

[profiler]

[resource]

[revoke]

[role]

[saml]

[security_compliance]

[shadow_users]

[signing]

[token]

provider = fernet

[tokenless_auth]

[trust]
----
<1> Contraseña del usuario Keystone y nombre del nodo de control
****

. Reiniciar MySQL

+
[source, bash]
----
# service mysql restart
----

. Inicializar la base de datos Keystone:

+
[source, bash]
----
# su -s /bin/sh -c "keystone-manage db_sync" keystone
----

. Inicializar los repositorios de claves Fernet

+
[source, bash]
----
# keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
# keystone-manage credential_setup --keystone-user keystone --keystone-group keystone
----

. Iniciar los servicios de Keystone

+
[source, bash]
----
keystone-manage bootstrap --bootstrap-password {{ admin_pass}} --bootstrap-admin-url http://{{ nodes_by_name.controller.management_ip }}:35357/v3/ --bootstrap-internal-url http://{{ nodes_by_name.controller.tunnel_ip }}:5000/v3/ --bootstrap-public-url http://{{ nodes_by_name.controller.provider_ip}}:5000/v3/ --bootstrap-region-id {{ region }} <1>
----
<1> Completar con la contraseña de `admin`, las direcciones IP del nodo de control y el nombre de la región (p.e. `RegionOne`)

. Configurar el archivo `/etc/apache2/apache2.conf`

+
.El archivo `/etc/apache2/apache2.conf`
****
[source, bash]
----
Mutex file:${APACHE_LOCK_DIR} default

PidFile ${APACHE_PID_FILE}

Timeout 300

KeepAlive On

MaxKeepAliveRequests 100

KeepAliveTimeout 5

User ${APACHE_RUN_USER}
Group ${APACHE_RUN_GROUP}

HostnameLookups Off

ErrorLog ${APACHE_LOG_DIR}/error.log

LogLevel warn

IncludeOptional mods-enabled/*.load
IncludeOptional mods-enabled/*.conf

Include ports.conf

<Directory />
	Options FollowSymLinks
	AllowOverride None
	Require all denied
</Directory>

<Directory /usr/share>
	AllowOverride None
	Require all granted
</Directory>

<Directory /var/www/>
	Options Indexes FollowSymLinks
	AllowOverride None
	Require all granted
</Directory>

AccessFileName .htaccess

<FilesMatch "^\.ht">
	Require all denied
</FilesMatch>

LogFormat "%v:%p %h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" vhost_combined
LogFormat "%h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" combined
LogFormat "%h %l %u %t \"%r\" %>s %O" common
LogFormat "%{Referer}i -> %U" referer
LogFormat "%{User-agent}i" agent

IncludeOptional conf-enabled/*.conf

IncludeOptional sites-enabled/*.conf

ServerName {{ nodes.controller.name }} <1>
----
<1> Configurar `ServerName` con el nombre del nodo de control
****

. Reiniciar Apache

+
[source, bash]
----
# service apache2 restart
----

. Eliminar la base de datos SQLite predetermianda

+
[source, bash]
----
# rm -rf /var/lib/keystone/keystone.db
----

. Configurar el archivo de credenciales del usuario `admin`

+
[source, bash]
----
export OS_USERNAME=admin
export OS_PASSWORD={{ admin_pass }} <1>
export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://{{ nodes.controller.name }}:35357/v3 <2>
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2
export OS_AUTH_TYPE=password
----
<1> Contraseña de `admin`
<2> Nombre del nodo de control

. Configurar el archivo de credenciales del usuario `demo`

+
[source, bash]
----
export OS_USERNAME=demo
export OS_PASSWORD={{ demo_pass }} <1>
export OS_PROJECT_NAME=demo
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://{{ nodes.controller.name }}:5000/v3 <2>
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2
export OS_AUTH_TYPE=password
----
<1> Contraseña de `demo`
<2> Nombre del nodo de control

. Realizar la configuración de Keystone (dominio `default`, proyectos `service` y `demo`, usuario `demo`, rol `user` y añadir el usuario `demo` al proyecto `demo` con el rol `user`)

+
[source, bash]
----
# source openrc-admin <1>

# openstack domain create --description "Default Domain" default
# openstack project create --domain default --description "Service Project" service
# openstack project create --domain default --description "Demo Project" demo
# openstack user create --domain default demo --password {{ demo_pass }} <2>
# openstack role create user
# openstack role add --project demo --user demo user
----
<1> Cargar las credenciales de `admin`
<2> Contraseña del usuario `demo`

. Configurar el archivo `/etc/keystone/keystone-paste.ini`

.El archivo `/etc/keystone/keystone-paste.ini`
****
[source, bash]
----
[filter:debug]
use = egg:oslo.middleware#debug

[filter:request_id]
use = egg:oslo.middleware#request_id

[filter:build_auth_context]
use = egg:keystone#build_auth_context

[filter:token_auth]
use = egg:keystone#token_auth

[filter:admin_token_auth]
use = egg:keystone#admin_token_auth

[filter:json_body]
use = egg:keystone#json_body

[filter:cors]
use = egg:oslo.middleware#cors
oslo_config_project = keystone

[filter:http_proxy_to_wsgi]
use = egg:oslo.middleware#http_proxy_to_wsgi

[filter:healthcheck]
use = egg:oslo.middleware#healthcheck

[filter:ec2_extension]
use = egg:keystone#ec2_extension

[filter:ec2_extension_v3]
use = egg:keystone#ec2_extension_v3

[filter:s3_extension]
use = egg:keystone#s3_extension

[filter:url_normalize]
use = egg:keystone#url_normalize

[filter:sizelimit]
use = egg:oslo.middleware#sizelimit

[filter:osprofiler]
use = egg:osprofiler#osprofiler

[app:public_service]
use = egg:keystone#public_service

[app:service_v3]
use = egg:keystone#service_v3

[app:admin_service]
use = egg:keystone#admin_service

[pipeline:public_api]
pipeline = healthcheck cors sizelimit http_proxy_to_wsgi osprofiler url_normalize request_id build_auth_context token_auth json_body ec2_extension public_service

[pipeline:admin_api]
pipeline = healthcheck cors sizelimit http_proxy_to_wsgi osprofiler url_normalize request_id build_auth_context token_auth json_body ec2_extension s3_extension admin_service

[pipeline:api_v3]
pipeline = healthcheck cors sizelimit http_proxy_to_wsgi osprofiler url_normalize request_id build_auth_context token_auth json_body ec2_extension_v3 s3_extension service_v3

[app:public_version_service]
use = egg:keystone#public_version_service

[app:admin_version_service]
use = egg:keystone#admin_version_service

[pipeline:public_version_api]
pipeline = healthcheck cors sizelimit osprofiler url_normalize public_version_service

[pipeline:admin_version_api]
pipeline = healthcheck cors sizelimit osprofiler url_normalize admin_version_service

[composite:main]
use = egg:Paste#urlmap
/v2.0 = public_api
/v3 = api_v3
/ = public_version_api

[composite:admin]
use = egg:Paste#urlmap
/v2.0 = admin_api
/v3 = api_v3
/ = admin_version_api

----
****

## Instalación de Glance

La instalación de Glance se realiza en el nodo de control.

. Creación y configuración de la base de datos `glance`

+
[source, bash]
----
MariaDB [(none)]> CREATE DATABASE glance;

MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \
  IDENTIFIED BY 'GLANCE_DBPASS'; <1>
MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \
  IDENTIFIED BY 'GLANCE_DBPASS'; <2>
----
<1> Contraseña del usuario Glance
<2> Contraseña del usuario Glance


. Realizar la configuración de Glance (usuario `glance`, añadir el usuario `glance` al proyecto `service` con el rol `admin` y crear el servicio `glance`)

+
[source, bash]
----
# source openrc-admin <1>

# openstack user create --domain default glance --password {{ glance_dbpass }} <1>
# openstack role add --project service --user glance admin
# openstack service create --name glance --description "OpenStack Image" image
----
<1> Cargar las credenciales de `admin`
<2> Contraseña del usuario `glance`

. Crear los endpoints de la API

+
[source, bash]
----
# openstack endpoint create --region {{region}} image public http://{{ nodes_by_name.controller.provider_ip }}:9292 <1>
# openstack endpoint create --region {{region}} image internal http://{{ nodes_by_name.controller.tunnel_ip }}:9292 <2>
# openstack endpoint create --region {{region}} image admin http://{{ nodes_by_name.controller.management_ip }}:9292 <3>
----
<1> Región (p.e, `RegionOne`) e IP externa del nodo de control
<2> Región (p.e, `RegionOne`) e IP de túnel del nodo de control
<3> Región (p.e, `RegionOne`) e IP de mantenimiento del nodo de control


. Instalar los paquetes de Glance 

+
[source, bash]
----
# apt install glance
----

. Configurar el archivo `/etc/glance/glance-api.conf`

+
.El archivo `/etc/glance/glance-api.conf`
****
[source, bash]
----
[DEFAULT]

transport_url = rabbit://openstack:{{ RABBIT_PASS }}@{{ nodes.controller.name }} <1>

[cors]

[cors.subdomain]

[database]

sqlite_db = /var/lib/glance/glance.sqlite

backend = sqlalchemy

connection = mysql+pymysql://glance:{{ glance_dbpass }}@{{ nodes.controller.name }}/glance <2>

[glance_store]

stores = file,http
default_store = file
filesystem_store_datadir = {{ glance_image_dir }} <3>

[image_format]

disk_formats = ami,ari,aki,vhd,vhdx,vmdk,raw,qcow2,vdi,iso,ploop.root-tar

[keystone_authtoken]

auth_uri = http://{{ nodes.controller.name }}:5000 <4>
auth_url = http://{{ nodes.controller.name }}:35357 <5>
memcached_servers = {{ nodes.controller.name }}:11211 <6>
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = {{ glance_dbpass }} <7>

[matchmaker_redis]

[oslo_concurrency]

[oslo_messaging_amqp]

[oslo_messaging_kafka]

[oslo_messaging_notifications]

driver = messagingv2

[oslo_messaging_rabbit]

[oslo_messaging_zmq]

[oslo_middleware]

[oslo_policy]

[paste_deploy]

flavor = keystone

[profiler]

[store_type_location_strategy]

[task]

[taskflow_executor]
----
<1> Contraseña de RabbitMQ y nombre del nodo de control
<2> Contraseña de Glance y nombre del nodo de control
<3> Directorio donde se vayan a almacenar las imágenes (p.e. `/var/lib/glance/images`)
<4> Nombre del nodo de control
<5> Nombre del nodo de control
<6> Nombre del nodo de control
<7> Contraseña de Glance

****

. Modificar el archivo `/etc/glance/glance-registry.conf`

.El archivo `/etc/glance/glance-registry.conf`
****
[source, bash]
----
[DEFAULT]

transport_url = rabbit://openstack:{{ RABBIT_PASS }}@{{ nodes.controller.name }} <1>

[database]

sqlite_db = /var/lib/glance/glance.sqlite

backend = sqlalchemy

connection = mysql+pymysql://glance:{{ glance_dbpass }}@{{ nodes.controller.name }}/glance <2>

[keystone_authtoken]

auth_uri = http://{{ nodes.controller.name }}:5000 <3>
auth_url = http://{{ nodes.controller.name }}:35357 <4>
memcached_servers = {{ nodes.controller.name }}:11211 <5>
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = {{ glance_dbpass }}

[matchmaker_redis]

[oslo_messaging_amqp]

[oslo_messaging_kafka]

[oslo_messaging_notifications]

driver = messagingv2

[oslo_messaging_rabbit]

[oslo_messaging_zmq]

[oslo_policy]

[paste_deploy]

flavor = keystone

[profiler]
----
<1> Contraseña de RabbitMQ y nombre del nodo de control
<2> Contraseña de Glance y nombre del nodo de control
<3> Nombre del nodo de control
<4> Nombre del nodo de control
<5> Nombre del nodo de control
<6> Contraseña de Glance
****

## Instalación de Nova

### Instalación de Nova en el nodo de control

Realizar estar operaciones en el nodo de control

. Creación y configuración de la base de datos `nova`

+
[source, bash]
----
MariaDB [(none)]> CREATE DATABASE nova_api;
MariaDB [(none)]> CREATE DATABASE nova;
MariaDB [(none)]> CREATE DATABASE nova_cell0;

MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' \
  IDENTIFIED BY 'NOVA_DBPASS'; <1>
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' \
  IDENTIFIED BY 'NOVA_DBPASS'; <2>

MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \
  IDENTIFIED BY 'NOVA_DBPASS'; <3>
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \
  IDENTIFIED BY 'NOVA_DBPASS'; <4>

MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' \
  IDENTIFIED BY 'NOVA_DBPASS'; <5>
MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' \
  IDENTIFIED BY 'NOVA_DBPASS'; <6>

----
<1> Contraseña del usuario Nova
<2> Contraseña del usuario Nova
<3> Contraseña del usuario Nova
<4> Contraseña del usuario Nova
<5> Contraseña del usuario Nova
<6> Contraseña del usuario Nova

. Realizar la configuración de Nova (usuario `nova`, añadir el usuario `nova` al proyecto `service` con el rol `admin` y crear el servicio `nova`)

+
[source, bash]
----
# source openrc-admin <1>

# openstack user create --domain default nova --password {{ nova_dbpass }} <2>
# openstack role add --project service --user nova admin
# openstack service create --name nova --description "OpenStack Compute" compute
----
<1> Cargar las credenciales de `admin`
<2> Contraseña del usuario `nova`

. Crear los endpoints de la API

+
[source, bash]
----
# openstack endpoint create --region {{ region }} compute public http://{{ nodes_by_name.controller.provider_ip }}:8774/v2.1 <1>
# openstack endpoint create --region {{ region }} compute internal http://{{ nodes_by_name.controller.tunnel_ip }}:8774/v2.1 <2>
# openstack endpoint create --region {{ region }} compute admin http://{{ nodes_by_name.controller.management_ip }}:8774/v2.1 <3>
----
<1> Región (p.e, `RegionOne`) e IP externa del nodo de control
<2> Región (p.e, `RegionOne`) e IP de túnel del nodo de control
<3> Región (p.e, `RegionOne`) e IP de mantenimiento del nodo de control

. Realizar la configuración del servicio Placement (usuario `nova`, añadir el usuario `nova` al proyecto `service` con el rol `admin` y crear el servicio `nova`)

+
[source, bash]
----
# source openrc-admin <1>

# openstack user create --domain default placement --password {{ placement_pass }} <2>
# openstack role add --project service --user placement admin
# openstack service create --name placement --description "Placement API" placement
----
<1> Cargar las credenciales de `admin`
<2> Contraseña del usuario `placement`

. Crear los endpoints de la API

+
[source, bash]
----
# openstack endpoint create --region {{ region }} placement public http://{{ nodes_by_name.controller.provider_ip }}:8778 <1>
# openstack endpoint create --region {{ region }} placement internal http://{{ nodes_by_name.controller.tunnel_ip }}:8778 <2>
# openstack endpoint create --region {{ region }} placement admin http://{{ nodes_by_name.controller.management_ip }}:8778 <3>
----
<1> Región (p.e, `RegionOne`) e IP externa del nodo de control
<2> Región (p.e, `RegionOne`) e IP de túnel del nodo de control
<3> Región (p.e, `RegionOne`) e IP de mantenimiento del nodo de control

. Instalar los paquetes de Nova 

+
[source, bash]
----
# apt install nova-api nova-conductor nova-consoleauth \
  nova-novncproxy nova-scheduler nova-placement-api
----

. Configurar el archivo `/etc/nova/nova.conf`

+
.El archivo `/etc/nova/nova.conf`
****
[source, bash]
----
my_ip = {{ nodes.controller.management_ip }} <1>

use_neutron = True
firewall_driver = nova.virt.firewall.NoopFirewallDriver

dhcpbridge_flagfile=/etc/nova/nova.conf

dhcpbridge=/usr/bin/nova-dhcpbridge

linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver

force_dhcp_release=true

state_path=/var/lib/nova

enabled_apis=osapi_compute,metadata

transport_url = rabbit://openstack:{{ RABBIT_PASS }}@{{ nodes.controller.name }} <2>

[api]

auth_strategy = keystone

[api_database]

connection = mysql+pymysql://nova:{{ nova_dbpass }}@{{ nodes.controller.name }}/nova_api <3>

[barbican]

[cache]

[cells]

enable=False

[cinder]

os_region_name = {{region}} <4>

[cloudpipe]

[conductor]

[console]

[consoleauth]

[cors]

[cors.subdomain]

[crypto]

[database]

connection = mysql+pymysql://nova:{{ nova_dbpass }}@{{ nodes.controller.name }}/nova <5>

[ephemeral_storage_encryption]

[filter_scheduler]

[glance]

api_servers = http://{{ nodes.controller.name }}:9292 <6>

[guestfs]

[healthcheck]

[hyperv]

[image_file_url]

[ironic]

[key_manager]

[keystone_authtoken]

auth_uri = http://{{ nodes.controller.name }}:5000 <7>
auth_url = http://{{ nodes.controller.name }}:35357 <8>
memcached_servers = {{nodes.controller.name}}:11211 <9>
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = {{ nova_dbpass }} <10>

[libvirt]

[matchmaker_redis]

[metrics]

[mks]

[neutron]

url = http://{{ nodes.controller.name }}:9696 <11>
auth_url = http://{{ nodes.controller.name }}:35357 <12>
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = {{ region }} <13>
project_name = service
username = neutron
password = {{ neutron_dbpass }} <14>
service_metadata_proxy = true
metadata_proxy_shared_secret = {{ metadata_secret }} <15>

[notifications]

[osapi_v21]

[oslo_concurrency]

lock_path = /var/lib/nova/tmp

[oslo_messaging_amqp]

[oslo_messaging_kafka]

[oslo_messaging_notifications]

[oslo_messaging_rabbit]

[oslo_messaging_zmq]

[oslo_middleware]

[oslo_policy]

[pci]

[placement]

os_region_name = {{ region }} <16>
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = Default
auth_url = http://{{ nodes.controller.name }}:35357/v3 <17>
username = placement
password = {{ placement_pass }} <18>

[quota]

[rdp]

[remote_debug]

[scheduler]

periodic_task_interval=300

[serial_console]

[service_user]

[spice]

[ssl]

[trusted_computing]

[upgrade_levels]

[vendordata_dynamic_auth]

[vmware]

[vnc]

enabled = true
vncserver_listen = $my_ip
vncserver_proxyclient_address = $my_ip

[workarounds]

[wsgi]

api_paste_config=/etc/nova/api-paste.ini

[xenserver]

[xvp]

----
<1> IP del nodo de control
<2> Contraseña de RabbitMQ y nombre del nodo de control
<3> Contraseña de Nova y nombre del nodo de control
<4> Nombre de la región (p.e. `RegionOne`)
<5> Contraseña de Nova y nombre del nodo de control
<6> Nombre del nodo de control
<7> Nombre del nodo de control
<8> Nombre del nodo de control
<9> Nombre del nodo de control
<10> Contraseña de Nova
<11> Nombre del nodo de control
<12> Nombre del nodo de control
<13> Nombre de la región (p.e. `RegionOne`)
<14> Contraseña de Neutron
<15> Secreto para metadatos
<16> Nombre de la región (p.e. `RegionOne`)
<17> Nombre del nodo de control
<18> Contraseña de Placement
****

. Crear las bases de datos y las celdas `cell0` y `cell1`

+
[source, bash]
----
# su -s /bin/sh -c "nova-manage api_db sync" nova
# su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
# su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova 109e1d4b-536a-40d0-83c6-5f121b82b650
# su -s /bin/sh -c "nova-manage db sync" nova
----

. Reiniciar los servicios

+
[source, bash]
----
# service nova-api restart
# service nova-consoleauth restart
# service nova-scheduler restart
# service nova-conductor restart
# service nova-novncproxy restart
----


### Instalación de Nova en los nodos de cómputo

Realizar estar operaciones en cada uno de los nodos de cómputo

. Instalar los paquetes de Nova 

+
[source, bash]
----
# apt install nova-compute
----

. Configurar el archivo `/etc/nova/nova.conf`

+
.El archivo `/etc/nova/nova.conf`
****
[source, bash]
----
[DEFAULT]

my_ip = {{ ansible_eth0.ipv4.address }} <1>
use_neutron = True
firewall_driver = nova.virt.firewall.NoopFirewallDriver

network_api_class = nova.network.neutronv2.api.API
security_group_api = neutron

instance_usage_audit_period=hour

instance_usage_audit=True

dhcpbridge_flagfile=/etc/nova/nova.conf

dhcpbridge=/usr/bin/nova-dhcpbridge

linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver

force_dhcp_release=true

state_path=/var/lib/nova

enabled_apis=osapi_compute,metadata

transport_url = rabbit://openstack:{{ RABBIT_PASS }}@{{ nodes.controller.name }} <2>

[api]

auth_strategy = keystone

[api_database]

connection = mysql+pymysql://nova:{{ nova_dbpass }}@{{ nodes.controller.name }}/nova_api <3>

[barbican]

[cache]

[cells]

enable=False

[cinder]

[cloudpipe]

[conductor]

[console]

[consoleauth]

[cors]

[cors.subdomain]

[crypto]

[database]

connection = mysql+pymysql://nova:{{ nova_dbpass }}@{{ nodes.controller.name }}/nova <4>

[ephemeral_storage_encryption]

[filter_scheduler]

[glance]

api_servers = http://{{ nodes.controller.name }}:9292 <5>

[guestfs]

[healthcheck]

[hyperv]

[image_file_url]

[ironic]

[key_manager]

[keystone_authtoken]

auth_uri = http://{{ nodes.controller.name }}:5000 <6>
auth_url = http://{{ nodes.controller.name }}:35357 <7>
memcached_servers = {{ nodes.controller.name }}:11211 <8>
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = {{ nova_dbpass }} <9>

[libvirt]

cpu_mode=custom

cpu_model=kvm64

[matchmaker_redis]

[metrics]

[mks]

[neutron]

url = http://{{ nodes.controller.name }}:9696 <10>
auth_url = http://{{ nodes.controller.name }}:35357 <11>
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = {{ region }} <12>
project_name = service
username = neutron
password = {{ neutron_dbpass }} <13>

[notifications]

notify_on_state_change=vm_and_task_state

[osapi_v21]

[oslo_concurrency]

lock_path = /var/lib/nova/tmp

[oslo_messaging_amqp]

[oslo_messaging_kafka]

[oslo_messaging_notifications]

driver = messagingv2

[oslo_messaging_rabbit]

[oslo_messaging_zmq]

[oslo_middleware]

[oslo_policy]

[pci]

[placement]

os_region_name = {{ region }} <14>
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = Default
auth_url = http://{{ nodes.controller.name }}:35357/v3 <15>
username = placement
password = {{ placement_pass }} <16>

[quota]

[rdp]

[remote_debug]

[scheduler]

[serial_console]

[service_user]

[spice]

[ssl]

[trusted_computing]

[upgrade_levels]

[vendordata_dynamic_auth]

[vmware]

[vnc]

enabled = True
vncserver_listen = 0.0.0.0
vncserver_proxyclient_address = $my_ip
novncproxy_base_url = http://{{ nodes_by_name.controller.provider_ip }}:6080/vnc_auto.html <17>

[workarounds]

[wsgi]

api_paste_config=/etc/nova/api-paste.ini

[xenserver]

[xvp]
----
<1> IP del nodo de cómputo
<2> Contraseña de RabbitMQ y nombre del nodo de control
<3> Contraseña de Nova y nombre del nodo de control
<4> Contraseña de Nova y nombre del nodo de control
<5> Nombre del nodo de control
<6> Nombre del nodo de control
<7> Nombre del nodo de control
<8> Nombre del nodo de control
<9> Contraseña de Nova
<10> Nombre del nodo de control
<11> Nombre del nodo de control
<12> Nombre de la región (p.e. `RegionOne`)
<13> Contraseña de Neutron
<14> Nombre de la región (p.e. `RegionOne`)
<15> Nombre del nodo de control
<16> IP externa del nodo de control
****

### Configuración de Nova en el nodo de control

Realizar estar operaciones en el nodo de control

. Crear los sabores

+
[source, bash]
----
# source openrc-admin <1>

# openstack flavor create --vcpus 1 --ram 512 --disk 1 tiny
# openstack flavor create --vcpus 1 --ram 2048 --disk 20 small
# openstack flavor create --vcpus 2 --ram 4096 --disk 40 medium
# openstack flavor create --vcpus 4 --ram 8192 --disk 80 large
# openstack flavor create --vcpus 8 --ram 16384 --disk 160 xlarge
----
<1> Cargar las credenciales de `admin`

. Descubrir los servidores de cómputo

+
[source, bash]
----
# su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova'
----

## Neutron

### Instalación de Neutron en el nodo de control

Realizar estar operaciones en el nodo de control

. Creación y configuración de la base de datos `neutron`

+
[source, bash]
----
MariaDB [(none)]> CREATE DATABASE neutron;

MariaDB [(none)]> GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' \
  IDENTIFIED BY 'NEUTRON_DBPASS'; <1>
MariaDB [(none)]> GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' \
  IDENTIFIED BY 'NEUTRON_DBPASS'; <2>
Exit

----
<1> Contraseña del usuario Neutron
<2> Contraseña del usuario Neutron

. Realizar la configuración de Neutron (usuario `neutron`, añadir el usuario `neutron` al proyecto `service` con el rol `admin` y crear el servicio `neutron`)

+
[source, bash]
----
# source openrc-admin <1>

# openstack user create --domain default neutron --password {{ neutron_dbpass }} <2>
# openstack role add --project service --user neutron admin
# openstack service create --name neutron --description "OpenStack Networking" network
----
<1> Cargar las credenciales de `admin`
<2> Contraseña del usuario `nova`

. Crear los endpoints de la API

+
[source, bash]
----
# openstack endpoint create --region {{region}} neutron public http://{{ nodes_by_name.controller.provider_ip }}:9696 <1>
# openstack endpoint create --region {{region}} neutron internal http://{{ nodes_by_name.controller.tunnel_ip }}:9696 <2>
# openstack endpoint create --region {{region}} neutron admin http://{{ nodes_by_name.controller.management_ip }}:9696 <3>
----
<1> Región (p.e, `RegionOne`) e IP externa del nodo de control
<2> Región (p.e, `RegionOne`) e IP de túnel del nodo de control
<3> Región (p.e, `RegionOne`) e IP de mantenimiento del nodo de control

. Instalar los paquetes de Neutron 

+
[source, bash]
----
# apt install neutron-server neutron-plugin-ml2
----

. Configurar el archivo `/etc/neutron/neutron.conf`

+
.El archivo `/etc/neutron/neutron.conf`
****
[source, bash]
----
[DEFAULT]

auth_strategy = keystone

core_plugin = ml2
service_plugins = router,neutron_lbaas.services.loadbalancer.plugin.LoadBalancerPluginv2
allow_overlapping_ips = true
transport_url = rabbit://openstack:{{ RABBIT_PASS }}@{{ nodes.controller.name }} <1>

notify_nova_on_port_status_changes = true

notify_nova_on_port_data_changes = true

dhcp_agents_per_network = {{ compute_nodes_quantity }} <2>

[agent]

root_helper = sudo /usr/bin/neutron-rootwrap /etc/neutron/rootwrap.conf

[cors]

[cors.subdomain]

[database]

connection = mysql+pymysql://neutron:{{ neutron_dbpass }}@{{ nodes.controller.name }}/neutron <3>

[keystone_authtoken]

auth_uri = http://{{ nodes.controller.name }}:5000 <4>
auth_url = http://{{ nodes.controller.name }}:35357 <5>
memcached_servers = {{ nodes.controller.name }}:11211 <6>
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = {{ neutron_dbpass }} <7>

[matchmaker_redis]

[nova]

auth_url = http://{{ nodes.controller.name }}:35357 <8>
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = {{ region }} <9>
project_name = service
username = nova
password = {{ nova_dbpass }} <10>

[oslo_concurrency]

[oslo_messaging_amqp]

[oslo_messaging_kafka]

[oslo_messaging_notifications]

driver = messagingv2

[oslo_messaging_rabbit]

[oslo_messaging_zmq]

[oslo_middleware]

[oslo_policy]

[qos]

[quotas]

[ssl]

----
<1> Contraseña de RabbitMQ y nombre del nodo de control
<2> Cantidad de servidores de cómputo
<3> Contraseña de Neutron y nombre del nodo de control
<4> Nombre del nodo de control
<5> Nombre del nodo de control
<6> Nombre del nodo de control
<7> Contraseña de Neutron
<8> Nombre del nodo de control
<9> Nombre de la región (p.e. `RegionOne`)
<10> Contraseña de Nova
****

. Modificar el archivo `/etc/neutron/plugins/ml2/ml2_conf.ini`

+
.El archivo `/etc/neutron/plugins/ml2/ml2_conf.ini`
****
[source, bash]
----
[DEFAULT]

[ml2]

type_drivers = flat,vlan,vxlan

tenant_network_types = vxlan

mechanism_drivers = openvswitch,l2population

extension_drivers = port_security

[ml2_type_flat]

[ml2_type_geneve]

[ml2_type_gre]

[ml2_type_vlan]

[ml2_type_vxlan]

vni_ranges = 1:1000

[securitygroup]

firewall_driver = iptables_hybrid

enable_security_group = true

enable_ipset = true

----
****

. Poblar la base de datos de Neutron

+
[source, bash]
----
# su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron
----

. Reiniciar Neutron

[source, bash]
----
# service neutron-server restart
----

### Instalación de Neutron en el nodo de red

Realizar estos pasos en el nodo de red

[NOTE]
====
En este tutorial seguimos el https://docs.openstack.org/kilo/networking-guide/scenario_provider_ovs.html[escenario de _provider networks_ con OpenvSwich].
====

. Configurar el kernel para desactivar el _reverse path filtering_. Añadir estas líneas el archivo `/etc/sysctl.conf`

+
[source, bash]
----
net.ipv4.ip_forward=1
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0
----

. Cargar la nueva configuración del kernel

+
[source, bash]
----
# systcl -p
----

. Instalar Neutron

[source, bash]
----
# apt install neutron-openvswitch-agent neutron-l3-agent neutron-dhcp-agent neutron-metadata-agent
----

. Configurar el archivo `/etc/neutron/neutron.conf`

+
.El archivo `/etc/neutron/neutron.conf`
****
[source, bash]
----
[DEFAULT]

auth_strategy = keystone

core_plugin = ml2
service_plugins = router,neutron_lbaas.services.loadbalancer.plugin.LoadBalancerPluginv2
allow_overlapping_ips = true
transport_url = rabbit://openstack:{{ RABBIT_PASS }}@{{ nodes.controller.name }} <1>

notify_nova_on_port_status_changes = true

notify_nova_on_port_data_changes = true

dhcp_agents_per_network = {{ compute_nodes_quantity }} <2>

[agent]

root_helper = sudo /usr/bin/neutron-rootwrap /etc/neutron/rootwrap.conf

[cors]

[cors.subdomain]

[database]

connection = mysql+pymysql://neutron:{{ neutron_dbpass }}@{{ nodes.controller.name }}/neutron <3>

[keystone_authtoken]

auth_uri = http://{{ nodes.controller.name }}:5000 <4>
auth_url = http://{{ nodes.controller.name }}:35357 <5>
memcached_servers = {{ nodes.controller.name }}:11211 <6>
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = {{ neutron_dbpass }} <7>

[matchmaker_redis]

[nova]

auth_url = http://{{ nodes.controller.name }}:35357 <8>
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = {{ region }} <9>
project_name = service
username = nova
password = {{ nova_dbpass }} <10>

[oslo_concurrency]

[oslo_messaging_amqp]

[oslo_messaging_kafka]

[oslo_messaging_notifications]

driver = messagingv2

[oslo_messaging_rabbit]

[oslo_messaging_zmq]

[oslo_middleware]

[oslo_policy]

[qos]

[quotas]

[ssl]

----
<1> Contraseña de RabbitMQ y nombre del nodo de control
<2> Cantidad de servidores de cómputo
<3> Contraseña de Neutron y nombre del nodo de control
<4> Nombre del nodo de control
<5> Nombre del nodo de control
<6> Nombre del nodo de control
<7> Contraseña de Neutron
<8> Nombre del nodo de control
<9> Nombre de la región (p.e. `RegionOne`)
<10> Contraseña de Nova
****

. Modificar el archivo `/etc/neutron/l3_agent.ini`

+
.El archivo `/etc/neutron/l3_agent.ini`
****
[source, bash]
----
[DEFAULT]

interface_driver = openvswitch

agent_mode = legacy

handle_internal_only_routers = true

enable_metadata_proxy = true

external_network_bridge =

[agent]

[ovs]
----
****

. Modificar el archivo `/etc/neutron/dhcp_agent.ini`

+
.El archivo `/etc/neutron/dhcp_agent.ini`
****
[source, bash]
----
[DEFAULT]

ovs_integration_bridge = br-int

interface_driver = openvswitch

dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq

enable_isolated_metadata = true

dnsmasq_config_file = /etc/neutron/dnsmasq-neutron.conf

[agent]

[ovs]

----
****

. Modificar el archivo `/etc/neutron/dnsmasq-neutron.conf`

+
.El archivo `/etc/neutron/dnsmasq-neutron.conf`
****
[source, bash]
----
 -s
dhcp-option-force=26,1450
----
****

. Modificar el archivo `/etc/neutron/metadata_agent.ini`

+
.El archivo `/etc/neutron/metadata_agent.ini`
****
[source, bash]
----
[DEFAULT]

auth_uri = http://{{ nodes.controller.name }}:5000 <1>
auth_url = http://{{ nodes.controller.name }}:35357 <2>
auth_region = {{ region }} <3>
auth_plugin = password
project_domain_id = default
user_domain_id = default
project_name = service
username = neutron
password = {{ neutron_dbpass }} <4>

nova_metadata_ip = {{ nodes.controller.management_ip }} <5>

nova_metadata_port = 8775

metadata_proxy_shared_secret = {{ metadata_secret }} <6>

nova_metadata_protocol = http

[agent]

[cache]
----
<1> Nombre del nodo de control
<2> Nombre del nodo de control
<3> Nombre de la región (p.e. `RegionOne`)
<4> Contraseña de Neutron
<5> IP de mantenimiento del nodo de control
<6> Secreto para metadatos

****





. Modificar el archivo `/etc/neutron/plugins/ml2/openvswitch_agent.ini`

+
.El archivo `/etc/neutron/plugins/ml2/openvswitch_agent.ini`
****
[source, bash]
----
[ovs]
integration_bridge = br-int

int_peer_patch_port = patch-tun

local_ip = {{ ansible_eth1.ipv4.address }} <1>

bridge_mappings = provider:br-ex

[agent]

polling_interval = 15

tunnel_types = vxlan

l2_population = True

arp_responder = False

enable_distributed_routing = False

[securitygroup]
firewall_driver = iptables_hybrid

enable_security_group = True

----
<1> IP de la red de túnel del nodo de red
****

. Reiniciar el servicio `openvswitch-switch`

+
[source, bash]
----
# service openvswitch-switch restart
----

. Añadir el bridge externo

+
[source, bash]
----
# ovs-vsctl add-br br-ex
----

. Añadir puerto al bridge externo

+
[source, bash]
----
# ovs-vsctl add-port br-ex {{ provider_interface }} <1>
----
<1> Nombre de la interfaz de red externa en el nodo de red

. Añadir el bridge interno

+
[source, bash]
----
# ovs-vsctl add-br br-int
----

. Crear el siguiente script en `/root/br-ex_setup.sh` para configurar la interfaz externa en el nodo de red. Ejecutar el script.

+
.El archivo  `/root/br-ex_setup.sh`
****
[source, bash]
----
/sbin/ip route |grep default |grep br-ex

if [ $? -ne 0 ]; then
    /sbin/ip route del default
    /sbin/ip addr del {{ nodes_by_name.network.provider_ip }}/24 dev {{ provider_interface }}
    /sbin/ip link set br-ex up
    /sbin/ip link set {{ provider_interface }} promisc on
    /sbin/ip addr add {{ nodes_by_name.network.provider_ip }}/24 dev br-ex
    /sbin/ip route add default via {{ provider_gateway }}
fi
----
****

. Configurar el archivo `/etc/network/interfaces` para añadir el bridge externo

+
.El archivo `/etc/network/interfaces`
****
[source, bash]
----
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

# The loopback network interface
auto lo
iface lo inet loopback

# The management network interface
auto {{management_interface}} <1>
iface {{management_interface}} inet static <2>
  address {{ nodes_by_name.network.management_ip }} <3>
  netmask {{ management_mask }} <4>
  network {{management_network}} <5>
  mtu {{ MTU }} <6>

# The tunnel network interface
auto {{tunnel_interface}} <7>
iface {{tunnel_interface}} inet static <8>
  address {{ nodes_by_name.network.tunnel_ip }} <9>
  netmask {{ tunnel_mask }} <10>
  network {{tunnel_network}} <11>
  mtu {{ MTU }} <12>

auto br-ex
allow-ovs br-ex
iface br-ex inet static
  address {{ nodes_by_name.network.provider_ip }} <13>
  netmask {{ provider_mask }} <14>
  gateway {{ provider_gateway }} <15>
  dns-nameservers {{ dns }} <16>
  ovs_type OVSBridge
  ovs_ports {{ provider_interface }} <17>

allow-br-ex {{ provider_interface }} <18>
iface {{ provider_interface }} inet manual <19>
   ovs_bridge br-ex
  ovs_type OVSPort
  up ip link set $IFACE promisc on
  down ip link set $IFACE promisc off
----
<1> Nombre de la interfaz de mantenimiento del nodo de red
<2> Nombre de la interfaz de mantenimiento del nodo de red
<3> Dirección IP de mantenimiento del nodo de red
<4> Máscara de red la red de mantenimiento
<5> Red de mantenimiento
<6> MTU
<7> Nombre de la interfaz de túnel del nodo de red
<8> Nombre de la interfaz de túnel del nodo de red
<9> Dirección IP de túnel del nodo de red
<10> Máscara de red la red de túnel
<11> Red de mantenimiento
<12> MTU
<13> Dirección IP externa del nodo de red
<14> Máscara de red la red externa
<15> Gateway de la red external
<16> IP del DNS
<17> Nombre de la interfaz de red externa
<18> Nombre de la interfaz de red externa
<19> Nombre de la interfaz de red externa
****

. Reiniciar los servicios de Neutron

+
[source, bash]
----
# service neutron-openvswitch-agent restart
# service neutron-dhcp-agent restart
# service neutron-metadata-agent restart
# service neutron-l3-agent restart
----

### Instalación de Neutron en los nodos de cómputo

Realizar estos pasos en cada uno de los nodos de cómputo

. Configurar el kernel para desactivar el _reverse path filtering_. Añadir estas líneas el archivo `/etc/sysctl.conf`

+
[source, bash]
----
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0
----

. Cargar la nueva configuración del kernel

+
[source, bash]
----
# systcl -p
----

. Instalar Neutron

+
[source, bash]
----
# apt install neutron-openvswitch-agent
----

. Configurar el archivo `/etc/neutron/neutron.conf`

+
.El archivo `/etc/neutron/neutron.conf`
****
[source, bash]
----
[DEFAULT]

auth_strategy = keystone

core_plugin = ml2
service_plugins = router,neutron_lbaas.services.loadbalancer.plugin.LoadBalancerPluginv2
allow_overlapping_ips = true
transport_url = rabbit://openstack:{{ RABBIT_PASS }}@{{ nodes.controller.name }} <1>

notify_nova_on_port_status_changes = true

notify_nova_on_port_data_changes = true

dhcp_agents_per_network = {{ compute_nodes_quantity }} <2>

[agent]

root_helper = sudo /usr/bin/neutron-rootwrap /etc/neutron/rootwrap.conf

[cors]

[cors.subdomain]

[database]

connection = mysql+pymysql://neutron:{{ neutron_dbpass }}@{{ nodes.controller.name }}/neutron <3>

[keystone_authtoken]

auth_uri = http://{{ nodes.controller.name }}:5000 <4>
auth_url = http://{{ nodes.controller.name }}:35357 <5>
memcached_servers = {{ nodes.controller.name }}:11211 <6>
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = {{ neutron_dbpass }} <7>

[matchmaker_redis]

[nova]

auth_url = http://{{ nodes.controller.name }}:35357 <8>
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = {{ region }} <9>
project_name = service
username = nova
password = {{ nova_dbpass }} <10>

[oslo_concurrency]

[oslo_messaging_amqp]

[oslo_messaging_kafka]

[oslo_messaging_notifications]

driver = messagingv2

[oslo_messaging_rabbit]

[oslo_messaging_zmq]

[oslo_middleware]

[oslo_policy]

[qos]

[quotas]

[ssl]

----
<1> Contraseña de RabbitMQ y nombre del nodo de control
<2> Cantidad de servidores de cómputo
<3> Contraseña de Neutron y nombre del nodo de control
<4> Nombre del nodo de control
<5> Nombre del nodo de control
<6> Nombre del nodo de control
<7> Contraseña de Neutron
<8> Nombre del nodo de control
<9> Nombre de la región (p.e. `RegionOne`)
<10> Contraseña de Nova
****

. Configurar el archivo `/etc/neutron/plugins/ml2/openvswitch_agent.ini`

+
.El archivo `/etc/neutron/plugins/ml2/openvswitch_agent.ini`
****
[source, bash]
----
[ovs]
integration_bridge = br-int

int_peer_patch_port = patch-tun

local_ip = {{ ansible_eth0.ipv4.address }} <1>

bridge_mappings = provider:br-ex

[agent]

polling_interval = 15

tunnel_types = vxlan

l2_population = True

arp_responder = False

enable_distributed_routing = False

[securitygroup]
firewall_driver = iptables_hybrid

enable_security_group = True

----
<1> IP de mantenimiento del nodo de cómputo
****

. Reiniciar el agente OpenvSwitch

+
[source, bash]
----
# service neutron-openvswitch-agent restart
----

### Configuración de la red

Realizar estas operaciones en el nodo de control.

. Crear la red externa

+
[source, bash]
----
# openstack network create  --share --external --provider-physical-network provider --provider-network-type flat {{network_name}} <1>
----
<1> Nombre de la red externa

. Crear la subred de la red externa

[source, bash]
----
# openstack subnet create --network {{network_name}} \ <1>
    --allocation-pool start={{allocation_pool_start}},end={{allocation_pool_end}} \ <2>
    --dns-nameserver {{dns_name_servers}} \ <3>
    --gateway {{provider_gateway}} \ <4>
    --subnet-range {{subnet_range}} \ <5>
    {{subnet_name}} <6>
----
<1> Nombre de la red externa
<2> Direcciones IP inicial y final del pool de direcciones asignadas
<4> IP de servidores DNS
<5> Gateway de la red externa
<6> Nombre de la subred

## Horizon

Realizar estos pasos en el nodo de control.

. Instalar Neutron

+
[source, bash]
----
# apt install openstack-dashboard
----

. Configurar el archivo `/etc/openstack-dashboard/local_settings.py`

+
.El archivo `/etc/openstack-dashboard/local_settings.py`
****
[source, bash]
----
import os

from django.utils.translation import ugettext_lazy as _

from horizon.utils import secret_key

from openstack_dashboard.settings import HORIZON_CONFIG

DEBUG = False

WEBROOT = '/'

ALLOWED_HOSTS = ['*']

OPENSTACK_API_VERSIONS = {
    "identity": 3,
    "image": 2,
    "volume": 2,
}

OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True

OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = 'Default'

LOCAL_PATH = os.path.dirname(os.path.abspath(__file__))

SECRET_KEY = secret_key.generate_or_read_from_file('/var/lib/openstack-dashboard/secret_key')

SESSION_ENGINE = 'django.contrib.sessions.backends.cache'

CACHES = {
    'default': {
         'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
         'LOCATION': '{{ nodes.controller.name }}:11211', <1>
    }
}

EMAIL_BACKEND = 'django.core.mail.backends.console.EmailBackend'

OPENSTACK_HOST = "{{ nodes.controller.name }}" <2>
OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST

OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"

OPENSTACK_KEYSTONE_BACKEND = {
    'name': 'native',
    'can_edit_user': True,
    'can_edit_group': True,
    'can_edit_project': True,
    'can_edit_domain': True,
    'can_edit_role': True,
}

OPENSTACK_HYPERVISOR_FEATURES = {
    'can_set_mount_point': False,
    'can_set_password': False,
    'requires_keypair': False,
    'enable_quotas': True
}

OPENSTACK_CINDER_FEATURES = {
    'enable_backup': False,
}

OPENSTACK_NEUTRON_NETWORK = {
    'enable_router': True,
    'enable_quotas': True,
    'enable_ipv6': True,
    'enable_distributed_router': False,
    'enable_ha_router': False,
    'enable_lb': True,
    'enable_firewall': True,
    'enable_vpn': True,
    'enable_fip_topology_check': True,

    # Default dns servers you would like to use when a subnet is
    # created.  This is only a default, users can still choose a different
    # list of dns servers when creating a new subnet.
    # The entries below are examples only, and are not appropriate for
    # real deployments
    # 'default_dns_nameservers': ["8.8.8.8", "8.8.4.4", "208.67.222.222"],

    # The profile_support option is used to detect if an external router can be
    # configured via the dashboard. When using specific plugins the
    # profile_support can be turned on if needed.
    'profile_support': None,
    #'profile_support': 'cisco',

    # Set which provider network types are supported. Only the network types
    # in this list will be available to choose from when creating a network.
    # Network types include local, flat, vlan, gre, vxlan and geneve.
    # 'supported_provider_types': ['*'],

    # You can configure available segmentation ID range per network type
    # in your deployment.
    # 'segmentation_id_range': {
    #     'vlan': [1024, 2048],
    #     'vxlan': [4094, 65536],
    # },

    # You can define additional provider network types here.
    # 'extra_provider_types': {
    #     'awesome_type': {
    #         'display_name': 'Awesome New Type',
    #         'require_physical_network': False,
    #         'require_segmentation_id': True,
    #     }
    # },

    # Set which VNIC types are supported for port binding. Only the VNIC
    # types in this list will be available to choose from when creating a
    # port.
    # VNIC types include 'normal', 'macvtap' and 'direct'.
    # Set to empty list or None to disable VNIC type selection.
    'supported_vnic_types': ['*'],
}

OPENSTACK_HEAT_STACK = {
    'enable_user_pass': True,
}

IMAGE_CUSTOM_PROPERTY_TITLES = {
    "architecture": _("Architecture"),
    "kernel_id": _("Kernel ID"),
    "ramdisk_id": _("Ramdisk ID"),
    "image_state": _("Euca2ools state"),
    "project_id": _("Project ID"),
    "image_type": _("Image Type"),
}

IMAGE_RESERVED_CUSTOM_PROPERTIES = []

API_RESULT_LIMIT = 1000
API_RESULT_PAGE_SIZE = 20

SWIFT_FILE_TRANSFER_CHUNK_SIZE = 512 * 1024

INSTANCE_LOG_LENGTH = 35

DROPDOWN_MAX_ITEMS = 30

TIME_ZONE = "Europe/Madrid"

AVAILABLE_THEMES = [
    ('default', 'Default', 'themes/default'),
    ('material', 'Material', 'themes/material'),
]

LOGGING = {
    'version': 1,
    # When set to True this will disable all logging except
    # for loggers specified in this configuration dictionary. Note that
    # if nothing is specified here and disable_existing_loggers is True,
    # django.db.backends will still log unless it is disabled explicitly.
    'disable_existing_loggers': False,
    'formatters': {
        'operation': {
            # The format of "%(message)s" is defined by
            # OPERATION_LOG_OPTIONS['format']
            'format': '%(asctime)s %(message)s'
        },
    },
    'handlers': {
        'null': {
            'level': 'DEBUG',
            'class': 'logging.NullHandler',
        },
        'console': {
            # Set the level to "DEBUG" for verbose output logging.
            'level': 'INFO',
            'class': 'logging.StreamHandler',
        },
        'operation': {
            'level': 'INFO',
            'class': 'logging.StreamHandler',
            'formatter': 'operation',
        },
    },
    'loggers': {
        # Logging from django.db.backends is VERY verbose, send to null
        # by default.
        'django.db.backends': {
            'handlers': ['null'],
            'propagate': False,
        },
        'requests': {
            'handlers': ['null'],
            'propagate': False,
        },
        'horizon': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'horizon.operation_log': {
            'handlers': ['operation'],
            'level': 'INFO',
            'propagate': False,
        },
        'openstack_dashboard': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'novaclient': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'cinderclient': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'keystoneclient': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'glanceclient': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'neutronclient': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'heatclient': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'swiftclient': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'openstack_auth': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'nose.plugins.manager': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'django': {
            'handlers': ['console'],
            'level': 'DEBUG',
            'propagate': False,
        },
        'iso8601': {
            'handlers': ['null'],
            'propagate': False,
        },
        'scss': {
            'handlers': ['null'],
            'propagate': False,
        },
    },
}

SECURITY_GROUP_RULES = {
    'all_tcp': {
        'name': _('All TCP'),
        'ip_protocol': 'tcp',
        'from_port': '1',
        'to_port': '65535',
    },
    'all_udp': {
        'name': _('All UDP'),
        'ip_protocol': 'udp',
        'from_port': '1',
        'to_port': '65535',
    },
    'all_icmp': {
        'name': _('All ICMP'),
        'ip_protocol': 'icmp',
        'from_port': '-1',
        'to_port': '-1',
    },
    'ssh': {
        'name': 'SSH',
        'ip_protocol': 'tcp',
        'from_port': '22',
        'to_port': '22',
    },
    'smtp': {
        'name': 'SMTP',
        'ip_protocol': 'tcp',
        'from_port': '25',
        'to_port': '25',
    },
    'dns': {
        'name': 'DNS',
        'ip_protocol': 'tcp',
        'from_port': '53',
        'to_port': '53',
    },
    'http': {
        'name': 'HTTP',
        'ip_protocol': 'tcp',
        'from_port': '80',
        'to_port': '80',
    },
    'pop3': {
        'name': 'POP3',
        'ip_protocol': 'tcp',
        'from_port': '110',
        'to_port': '110',
    },
    'imap': {
        'name': 'IMAP',
        'ip_protocol': 'tcp',
        'from_port': '143',
        'to_port': '143',
    },
    'ldap': {
        'name': 'LDAP',
        'ip_protocol': 'tcp',
        'from_port': '389',
        'to_port': '389',
    },
    'https': {
        'name': 'HTTPS',
        'ip_protocol': 'tcp',
        'from_port': '443',
        'to_port': '443',
    },
    'smtps': {
        'name': 'SMTPS',
        'ip_protocol': 'tcp',
        'from_port': '465',
        'to_port': '465',
    },
    'imaps': {
        'name': 'IMAPS',
        'ip_protocol': 'tcp',
        'from_port': '993',
        'to_port': '993',
    },
    'pop3s': {
        'name': 'POP3S',
        'ip_protocol': 'tcp',
        'from_port': '995',
        'to_port': '995',
    },
    'ms_sql': {
        'name': 'MS SQL',
        'ip_protocol': 'tcp',
        'from_port': '1433',
        'to_port': '1433',
    },
    'mysql': {
        'name': 'MYSQL',
        'ip_protocol': 'tcp',
        'from_port': '3306',
        'to_port': '3306',
    },
    'rdp': {
        'name': 'RDP',
        'ip_protocol': 'tcp',
        'from_port': '3389',
        'to_port': '3389',
    },
}

REST_API_REQUIRED_SETTINGS = ['OPENSTACK_HYPERVISOR_FEATURES',
                              'LAUNCH_INSTANCE_DEFAULTS',
                              'OPENSTACK_IMAGE_FORMATS',
                              'OPENSTACK_KEYSTONE_DEFAULT_DOMAIN']

 # The default theme if no cookie is present
DEFAULT_THEME = 'default'

WEBROOT='/horizon/'

ALLOWED_HOSTS = '*'

COMPRESS_OFFLINE = True

ALLOWED_PRIVATE_SUBNET_CIDR = {'ipv4': [], 'ipv6': []}
----
<1> Nombre del nodo de control
<2> Nombre del nodo de control
****

. Reiniciar Apache y Memcached

+
[source, bash]
----
# service apache2 reload
# service apache2 restart
# service memcached restart 
----

## Neutron LBaaS

### Instalación de Neutron LBaaS en el nodo de control

Realizar estos pasos en el nodo de control.

. Instalar `python-neutron-lbaas`
+
[source, bash]
----
# apt install python-neutron-lbaas
----

. Configurar el archivo `/etc/neutron/neutron_lbaas.conf`

+
.El archivo `/etc/neutron/neutron_lbaas.conf`
****
[source, bash]
----
[DEFAULT]

[certificates]

[quotas]

[service_auth]

[service_providers]

service_provider = LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default

----
****

. Realizar la migración de la base de datos `neutron-lbaas`

+
[source, bash]
----
# su -s /bin/sh -c "neutron-db-manage --subproject neutron-lbaas upgrade head"
----

. Reiniciar Neutron `neutron-server`

+
[source, bash]
----
# service neutron-server restart
----

. Clonar el repositorio de `neutron-lbaas`

+
[source, bash]
----
# git clone https://git.openstack.org/openstack/neutron-lbaas-dashboard /tmp/neutron-lbaas-dashboard
----

+
[NOTE]
====
Neutron LBaaS no tiene empaquetada la integración con Horizon. En estos casos descargaremos los fuentes, los procesaremos y los integraremos en Horizon.
====

. Instalar el dashboard de Neutron LBaaS desde `/tmp/neutron-lbaas-dashboard/`

+
[source, bash]
----
# cd /tmp/neutron-lbaas-dashboard/
# python setup.py install
----

. Copiar los archivos del dashboard de Neutron LBaaS

+
[source, bash]
----
# cp /tmp/neutron-lbaas-dashboard/neutron_lbaas_dashboard/enabled/_1481_project_ng_loadbalancersv2_panel.py /usr/share/openstack-dashboard/openstack_dashboard/enabled/
----

. Instalar el módulo `pexpect` con `pip` 

+
[source, bash]
----
pip install pexpect
----

. Realizar estos últimos pasos para terminar de integrar el dashboard en Horizon

+
[source, bash]
----
# cd /usr/share/openstack-dashboard
# python manage.py collectstatic <1>
# python manage.py compress
----
<1> Responder `yes` a la pregunta

. Reiniciar el servidor Apache

+
[source, bash]
----
# service apache2 restart
----

### Instalación de Neutron LBaaS en el nodo de red

Realizar estos pasos en el nodo de red.

. Instalar `neutron-lbaasv2-agent`
+
[source, bash]
----
# apt install neutron-lbaasv2-agent
----

. Configurar el archivo `/etc/neutron/lbaas_agent.ini`

+
.El archivo `/etc/neutron/lbaas_agent.ini`
****
[source, bash]
----
[DEFAULT]

device_driver = neutron_lbaas.drivers.haproxy.namespace_driver.HaproxyNSDriver
interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver

[haproxy]  
user_group = haproxy
----
****

. Configurar el archivo `/etc/neutron/neutron_lbaas.conf`

+
.El archivo `/etc/neutron/neutron_lbaas.conf`
****
[source, bash]
----
[DEFAULT]

[certificates]

[quotas]

[service_auth]

[service_providers]

service_provider = LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default
----
****

. Reiniciar el servicio neutron-lbaasv2-agent

+
[source, bash]
----
# service neutron-lbaasv2-agent restart
----
