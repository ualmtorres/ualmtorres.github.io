////
NO CAMBIAR!!
Codificación, idioma, tabla de contenidos, tipo de documento
////
:encoding: utf-8
:lang: es
:toc: right
:toc-title: Tabla de contenidos
:doctype: book
:imagesdir: ./images


////
Nombre y título del trabajo
////
# Instalación de OpenStack
Cloud-DI
Cloud-DI Team <clouddi@ual.es>

image::di.png[]

// NO CAMBIAR!! (Entrar en modo no numerado de apartados)
:numbered!: 


[abstract]
## Resumen

OpenStack es un software para la creación de clouds. El proyecto OpenStack ofrece gran cantidad de componentes para implementar los diferentes servicios que se quieran desplegar en nuestro cloud (máquinas virtuales, infraestructura de red, almacenamiento de bloques, almacenamiento de archivos compartidos, almacenamiento de objetos, orquestación, monitorización, por citar algunos). OpenStack tiene una arquitectura totalmente modular en la que partiendo de los componentes básicos (_Keystone_ para autenticación, _Glance_ para imágenes, _Neutron_ para redes y _Nova_ para instancias) podemos ir añadiendo los componentes adecuados para poder desplegar los servicios que queremos ofrecer.

En este tutorial se describen los pasos de instalación de los componentes que actualmente están disponibles en https://openstack.di.ual.es/horizon[OpenStack-DI].

[IMPORTANT]
====
El acceso a OpenStack-DI y a otros recursos más de Cloud-DI se puede hacer directamente desde https://openstack.di.ual.es/horizon[https://openstack.di.ual.es/horizon] o desde la web de servicios de Cloud-DI https://cloud.di.ual.es[https://cloud.di.ual.es]. Allí además hay disponible una lista de respuestas a preguntas frecuentes y los https://cloud.di.ual.es/TerminosServicio.html[Términos de servicio de Cloud-DI].

En https://moodle.di.ual.es[https://moodle.di.ual.es] se encuentran disponibles una serie de videos ilustrativos sobre los sevicios ofrecidos por Cloud-DI y sobre las operaciones básicas para el uso de OpenStack-DI.

Para realizar operaciones repetitivas o avanzadas es conveniente utilizar el CLI. Aquí está disponible la https://docs.openstack.org/python-openstackclient/pike/cli/command-list.html#command-list[Lista de comandos CLI].
====

// Entrar en modo numerado de apartados
:numbered:

//// 
COLOCA A CONTINUACION EL TITULO DEL APARTADO
////

## Componentes de OpenStack

OpenStack es un proyecto formado por distintos componentes. Cada uno de ellos añade una función a nuestro cloud (networking, almacenamiento, máquinas virtuales, orquestación, ...). Podemos clasificar estos componentes en básicos y opcionales.

* Componentes básicos 
** _Keystone_: Servicio de Identidad (*)
** _Neutron_: Servicio de redes (*)
** _Glance_: Servicio de imágenes (*)
** _Nova_: Servicio de cómputo (máquinas virtuales) (*)

* Componentes opcionales
** _Horizon_: Interfaz web (*)
** _Cinder_: Almacenamiento de bloques (*)
** _Manila_: Servicio de sistemas de archivos compartidos (*)
** _Swift_: Servicio de almacenamiento de objetos (*)
** _Sahara_: Servicio de procesamiento de datos (*)
** _Ceilometer_: Servicio de recolección de datos de telemetría
** _Aodh_: Servicio de alerta de telemetría
** _Heat_: Servicio de orquestación (*)
** _Barbican_: Servicio de gestión de claves (*)
** _Ironic_: Aprovisionamiento en máquinas físicas
** _Cloudkitty_: Servicio de tarificación

[NOTE]
====
Los componentes marcardos con asterisco (*) están disponibles actualmente en https://openstack.di.ual.es/horizon[OpenStack-DI].
====

La figura siguiente ilustra los componentes instalados en OpenStack-DI y su interacción básica. Los componentes en rojo son los componentes básicos.

.Componentes de OpenStack-DI
image::ComponentesOpenStack.png[]

* _Keystone_ proporciona servicios de identificación a todos los componentes OpenStack
* _Horizon_ proporciona un portal web de acceso al resto de componentes salvo a _Keystone_.
* _Glance_ proporciona las imágenes al crear las máquinas virtuales.
* _Neutron_ proporciona los servicios de networking a las máquinas virtuales
* _Cinder_ propociona almacenamiento de bloques a las máquinas virtuales. _Cinder_ también puede guardar snapshots de volumen.
* _Manila_ propociona servicios de almacenamiento compartido de archivos a las máquinas virtuales
* _Swift_ propociona almacenamiento de objetos a las máquinas virtuales y a _Sahara_. Opcionalmente se puede configurar _Nova_ para almacenar las imágenes en _Swift_.
* _Ceilometer_ recoge medidas de uso de los componentes de networking, imágenes, cómputo, almacenamiento y procesamiento de datos.
* _Heat_ permite la creación de stacks para la creación de infraestructura mediante código. Opcionalmente se puede combinar con _Ceilometer_ par ajustar dinánicamente la infraestructura en función del uso de recursos (RAM, cores, almacenamiento) recopilado por _Ceilometer_.

## Preparación del entorno

Para la instalación de los componentes de este tutorial partimos del siguiente escenario en el que contaremos con servidores dedicados para Control, Red y Cómputo. Los servicios de almacenamiento tienen los requisitos siguientes:

* _Cinder_: Almacenamiento en un NAS Synology y servicios ejecutándose en el nodo de Control.
* _Manila_: Servidor independiente.
* _Swift_: Dos servidores para proporcionar tolerancia a fallos.

La figura siguiente ilustra la arquitectura de referencia que usaremos en este tutorial. Tal y como aparece en la https://docs.openstack.org/ocata/install-guide-ubuntu/environment-networking.html[Guía de networking en la instalación de OpenStack] dispondremos de una red de mantenimiento, una red de túnel y la red externa. 

.Configuración y conexión de servidores
image::configuracionDeseable.png[]

Como se observa en la figura, todos los servidores están conectados a las redes de mantenimiento y túnel. Además, los servidores siguientes están contectados al exterior:

* Control: Proporciona acceso a la consola de _Horizon_ en la red de la UAL.
* Red: Ofrece conectividad a la red de la UAL a las máquinas virtuales.
* Almacenamiento compartido: Permite ofrecer sistemas de archivos de compartidos en la red de la UAL.

Los requisitos hardware mínimos de cada servidor son los que aparecen el la https://docs.openstack.org/ocata/install-guide-ubuntu/overview.html#example-architecture[arquitectura de ejemplo de la guía de instalación de OpenStack].

### Configuración de las interfaces de red

Es recomendable, aunque no necesario, una nomenclatura uniforme de las interfaces de red de los servidores que ofrecen la infraestructura a OpenStack. Si hay diferencias, recomendamos seguir la denominación clásica `eth0`, `eth1`, ... Sigue como `root` estos pasos cambiar los nombres de la interfaces de red a `eth0`, `eth1`, ...

1. Editar `/etc/default/grub` y cambiar la línea `GRUB_CMDLINE_LINUX=""` por  `GRUB_CMDLINE_LINUX="net.ifnames=0 biosdevname=0"`.
2. Actualizar GRUB con `update-grub`.
3. Actualizar el archivo `/etc/network/interfaces` con las interfaces de red ya a `eth0`, `eth1`, ...
4. Reiniciar el sistema con `reboot`

## Preparación de las máquinas

. En cada máquina crear un archivo `/etc/hosts` con las direcciones IP de la red de mantenimiento y los nombres que vayamos a dar a las máquinas:

+
[source, bash]
----
10.0.0.51 testcontroller

10.0.0.52 testnetwork

10.0.0.53 testcompute01
10.0.0.54 testcompute02
10.0.0.55 testcompute03
10.0.0.56 testcompute04

10.0.0.61 testobject01
10.0.0.62 testobject02

10.0.0.63 testshared
----
+

. Instalar `chrony` en todas las máquinas

+
[source, bash]
----
# apt-get install chrony
----
+

. Modificar en la máquina de control el archivo `/etc/chrony/chrony.conf`

+
.Archivo `/etc/chrony/chrony.conf` en el nodo de control
****
[source, bash]
----
pool 2.debian.pool.ntp.org offline iburst

server 1.es.pool.ntp.org iburst <1>
allow 10.0.0.0/24 <2>

keyfile /etc/chrony/chrony.keys

commandkey 1

driftfile /var/lib/chrony/chrony.drift

log tracking measurements statistics
logdir /var/log/chrony

maxupdateskew 100.0

dumponexit

dumpdir /var/lib/chrony

logchange 0.5

hwclockfile /etc/adjtime

rtcsync
----
<1> Servidor NTP
<2> Red de mantenimiento
****
+

. Modificar en el resto de máquinas el archivo `/etc/chrony/chrony.conf`

+
.Archivo `/etc/chrony/chrony.conf` en el resto de nodos
****
---
[source, bash]
----
server {{ nodes.controller.name }} iburst <1>

keyfile /etc/chrony/chrony.keys

commandkey 1

driftfile /var/lib/chrony/chrony.drift

log tracking measurements statistics
logdir /var/log/chrony

maxupdateskew 100.0

dumponexit

dumpdir /var/lib/chrony

logchange 0.5

hwclockfile /etc/adjtime

rtcsync
----
<1> Nombre del servidor de control
****
+

. Reiniciar `chrony` en todos los nodos

+
[source, bash]
----
# service chrony restart
----
+

. Añadir el repositorio de OpenStack Ocata en todos los nodos

+
[source, bash]
----
# apt-get install software-properties-common
# add-apt-repository cloud-archive:ocata
# apt update && apt dist-upgrade
----
+

. Instalar el cliente Python para OpenStack en todos los nodos

+
[source, bash]
----
# apt install python-openstackclient
----
+

. Instalar la base de datos en el nodo de control

+
[source, bash]
----
# apt-get install mariadb-server python-pymysql libmysqlclient-dev
----

. Modificar el archivo `/etc/mysql/mariadb.conf.d/99-openstack.cnf` en el nodo de control

+
.Archivo `/etc/mysql/mariadb.conf.d/99-openstack.cnf`
****
[source, bash]
----
[mysqld]
bind-address = {{ nodes.controller.management_ip }} <1>

default-storage-engine = innodb
innodb_file_per_table = on
max_connections = 4096
collation-server = utf8_general_ci
character-set-server = utf8
----
<1> Dirección IP de mantenimiento del nodo de control
****

. Modificar el archivo `/root/my.cnf` en el nodo de control

+
.Archivo `/root/my.cnf`
****
[source, bash]
----
[client]
user=root
password={{ mysql_root_password }} <1>
----
<1> Contraseña del usuario `root` de MySQL
****

+
[source, bash]
----
# service mysql restart
# mysql_secure_installation
----


. Instalar la cola de mensajes en el nodo de control

+
[source, bash]
----
# apt install rabbitmq-server
# rabbitmqctl add_user openstack {{ RABBIT_PASS }} <1>
# rabbitmqctl set_permissions openstack ".*" ".*" ".*"
----
<1> Contraseña de RabbitMQ

. Instalar Memcached en el nodo de control

+
[source, bash]
----
# apt install memcached python-memcache
----

+

. Modificar el archivo `/etc/memcached.conf`

+

.Archivo `/etc/memcached.conf`
****
[source, bash]
----
-d

logfile /var/log/memcached.log

-m 64

-p 11211

-u memcache

-l {{ nodes.controller.management_ip }} <1>
----
<1> Dirección IP de mantenimiento del nodo de control
****

. Reiniciar Memcached

+
[source, bash]
----
# service memcached restart
----

## Instalación de Keystone

La instalación de Keystone se realiza en el nodo de control

. Creación y configuración de la base de datos `keystone`

+
[source, bash]
----
MariaDB [(none)]> CREATE DATABASE keystone;
Grant proper access to the keystone database:

MariaDB [(none)]> GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \
IDENTIFIED BY {{ 'KEYSTONE_DBPASS' }}; <1>
MariaDB [(none)]> GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \
IDENTIFIED BY {{ 'KEYSTONE_DBPASS' }}; <2>
----
<1> Contraseña del usuario Keystone
<2> Contraseña del usuario Keystone

. Instalar los paquetes de Keystone 


+
[source, bash]
----
# apt install keystone
----

. Configurar el archivo `/etc/keystone.conf`

+
.El archivo `/etc/keystone.conf`
****
[source, bash]
----
[DEFAULT]

[assignment]

[auth]

[cache]

[catalog]

[cors]

[cors.subdomain]

[credential]

[database]

connection = mysql+pymysql://keystone:{{ keystone_dbpass }}@{{ nodes.controller.name }}/keystone <1>

[domain_config]

[endpoint_filter]

[endpoint_policy]

[eventlet_server]

[extra_headers]

[federation]

[fernet_tokens]

[healthcheck]

[identity]

[identity_mapping]

[kvs]

[ldap]

[matchmaker_redis]

[memcache]

[oauth1]

[oslo_messaging_amqp]

[oslo_messaging_kafka]

[oslo_messaging_notifications]

[oslo_messaging_rabbit]

[oslo_messaging_zmq]

[oslo_middleware]

[oslo_policy]

[paste_deploy]

[policy]

[profiler]

[resource]

[revoke]

[role]

[saml]

[security_compliance]

[shadow_users]

[signing]

[token]

provider = fernet

[tokenless_auth]

[trust]
----
<1> Contraseña del usuario Keystone y nombre del nodo de control
****

. Reiniciar MySQL

+
[source, bash]
----
# service mysql restart
----

. Inicializar la base de datos Keystone:

+
[source, bash]
----
# su -s /bin/sh -c "keystone-manage db_sync" keystone
----

. Inicializar los repositorios de claves Fernet

+
[source, bash]
----
# keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
# keystone-manage credential_setup --keystone-user keystone --keystone-group keystone
----

. Iniciar los servicios de Keystone

+
[source, bash]
----
keystone-manage bootstrap --bootstrap-password {{ admin_pass}} --bootstrap-admin-url http://{{ nodes_by_name.controller.management_ip }}:35357/v3/ --bootstrap-internal-url http://{{ nodes_by_name.controller.tunnel_ip }}:5000/v3/ --bootstrap-public-url http://{{ nodes_by_name.controller.provider_ip}}:5000/v3/ --bootstrap-region-id {{ region }} <1>
----
<1> Completar con la contraseña de `admin`, las direcciones IP del nodo de control y el nombre de la región (p.e. `RegionOne`)

. Configurar el archivo `/etc/apache2/apache2.conf`

+
.El archivo `/etc/apache2/apache2.conf`
****
[source, bash]
----
Mutex file:${APACHE_LOCK_DIR} default

PidFile ${APACHE_PID_FILE}

Timeout 300

KeepAlive On

MaxKeepAliveRequests 100

KeepAliveTimeout 5

User ${APACHE_RUN_USER}
Group ${APACHE_RUN_GROUP}

HostnameLookups Off

ErrorLog ${APACHE_LOG_DIR}/error.log

LogLevel warn

IncludeOptional mods-enabled/*.load
IncludeOptional mods-enabled/*.conf

Include ports.conf

<Directory />
	Options FollowSymLinks
	AllowOverride None
	Require all denied
</Directory>

<Directory /usr/share>
	AllowOverride None
	Require all granted
</Directory>

<Directory /var/www/>
	Options Indexes FollowSymLinks
	AllowOverride None
	Require all granted
</Directory>

AccessFileName .htaccess

<FilesMatch "^\.ht">
	Require all denied
</FilesMatch>

LogFormat "%v:%p %h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" vhost_combined
LogFormat "%h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" combined
LogFormat "%h %l %u %t \"%r\" %>s %O" common
LogFormat "%{Referer}i -> %U" referer
LogFormat "%{User-agent}i" agent

IncludeOptional conf-enabled/*.conf

IncludeOptional sites-enabled/*.conf

ServerName {{ nodes.controller.name }} <1>
----
<1> Configurar `ServerName` con el nombre del nodo de control
****

. Reiniciar Apache

+
[source, bash]
----
# service apache2 restart
----

. Eliminar la base de datos SQLite predetermianda

+
[source, bash]
----
# rm -rf /var/lib/keystone/keystone.db
----

. Configurar el archivo de credenciales del usuario `admin`

+
[source, bash]
----
export OS_USERNAME=admin
export OS_PASSWORD={{ admin_pass }} <1>
export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://{{ nodes.controller.name }}:35357/v3 <2>
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2
export OS_AUTH_TYPE=password
----
<1> Contraseña de `admin`
<2> Nombre del nodo de control

. Configurar el archivo de credenciales del usuario `demo`

+
[source, bash]
----
export OS_USERNAME=demo
export OS_PASSWORD={{ demo_pass }} <1>
export OS_PROJECT_NAME=demo
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://{{ nodes.controller.name }}:5000/v3 <2>
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2
export OS_AUTH_TYPE=password
----
<1> Contraseña de `demo`
<2> Nombre del nodo de control

. Realizar la configuración de Keystone (dominio `default`, proyectos `service` y `demo`, usuario `demo`, rol `user` y añadir el usuario `demo` al proyecto `demo` con el rol `user`)

+
[source, bash]
----
# source openrc-admin <1>

# openstack domain create --description "Default Domain" default
# openstack project create --domain default --description "Service Project" service
# openstack project create --domain default --description "Demo Project" demo
# openstack user create --domain default demo --password {{ demo_pass }} <2>
# openstack role create user
# openstack role add --project demo --user demo user
----
<1> Cargar las credenciales de `admin`
<2> Contraseña del usuario `demo`

. Configurar el archivo `/etc/keystone/keystone-paste.ini`

.El archivo `/etc/keystone/keystone-paste.ini`
****
[source, bash]
----
[filter:debug]
use = egg:oslo.middleware#debug

[filter:request_id]
use = egg:oslo.middleware#request_id

[filter:build_auth_context]
use = egg:keystone#build_auth_context

[filter:token_auth]
use = egg:keystone#token_auth

[filter:admin_token_auth]
use = egg:keystone#admin_token_auth

[filter:json_body]
use = egg:keystone#json_body

[filter:cors]
use = egg:oslo.middleware#cors
oslo_config_project = keystone

[filter:http_proxy_to_wsgi]
use = egg:oslo.middleware#http_proxy_to_wsgi

[filter:healthcheck]
use = egg:oslo.middleware#healthcheck

[filter:ec2_extension]
use = egg:keystone#ec2_extension

[filter:ec2_extension_v3]
use = egg:keystone#ec2_extension_v3

[filter:s3_extension]
use = egg:keystone#s3_extension

[filter:url_normalize]
use = egg:keystone#url_normalize

[filter:sizelimit]
use = egg:oslo.middleware#sizelimit

[filter:osprofiler]
use = egg:osprofiler#osprofiler

[app:public_service]
use = egg:keystone#public_service

[app:service_v3]
use = egg:keystone#service_v3

[app:admin_service]
use = egg:keystone#admin_service

[pipeline:public_api]
pipeline = healthcheck cors sizelimit http_proxy_to_wsgi osprofiler url_normalize request_id build_auth_context token_auth json_body ec2_extension public_service

[pipeline:admin_api]
pipeline = healthcheck cors sizelimit http_proxy_to_wsgi osprofiler url_normalize request_id build_auth_context token_auth json_body ec2_extension s3_extension admin_service

[pipeline:api_v3]
pipeline = healthcheck cors sizelimit http_proxy_to_wsgi osprofiler url_normalize request_id build_auth_context token_auth json_body ec2_extension_v3 s3_extension service_v3

[app:public_version_service]
use = egg:keystone#public_version_service

[app:admin_version_service]
use = egg:keystone#admin_version_service

[pipeline:public_version_api]
pipeline = healthcheck cors sizelimit osprofiler url_normalize public_version_service

[pipeline:admin_version_api]
pipeline = healthcheck cors sizelimit osprofiler url_normalize admin_version_service

[composite:main]
use = egg:Paste#urlmap
/v2.0 = public_api
/v3 = api_v3
/ = public_version_api

[composite:admin]
use = egg:Paste#urlmap
/v2.0 = admin_api
/v3 = api_v3
/ = admin_version_api

----
****

## Instalación de Glance

La instalación de Glance se realiza en el nodo de control.

. Creación y configuración de la base de datos `glance`

+
[source, bash]
----
MariaDB [(none)]> CREATE DATABASE glance;

MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \
  IDENTIFIED BY 'GLANCE_DBPASS'; <1>
MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \
  IDENTIFIED BY 'GLANCE_DBPASS'; <2>
----
<1> Contraseña del usuario Glance
<2> Contraseña del usuario Glance


. Realizar la configuración de Glance (usuario `glance`, añadir el usuario `glance` al proyecto `service` con el rol `admin` y crear el servicio `glance`)

+
[source, bash]
----
# source openrc-admin <1>

# openstack user create --domain default glance --password {{ glance_dbpass }} <1>
# openstack role add --project service --user glance admin
# openstack service create --name glance --description "OpenStack Image" image
----
<1> Cargar las credenciales de `admin`
<2> Contraseña del usuario `glance`

. Crear los endpoints de la API

+
[source, bash]
----
# openstack endpoint create --region {{region}} image public http://{{ nodes_by_name.controller.provider_ip }}:9292 <1>
# openstack endpoint create --region {{region}} image internal http://{{ nodes_by_name.controller.tunnel_ip }}:9292 <2>
# openstack endpoint create --region {{region}} image admin http://{{ nodes_by_name.controller.management_ip }}:9292 <3>
----
<1> Región (p.e, `RegionOne`) e IP externa del nodo de control
<2> Región (p.e, `RegionOne`) e IP de túnel del nodo de control
<3> Región (p.e, `RegionOne`) e IP de mantenimiento del nodo de control


. Instalar los paquetes de Glance 

+
[source, bash]
----
# apt install glance
----

. Configurar el archivo `/etc/glance/glance-api.conf`

+
.El archivo `/etc/glance/glance-api.conf`
****
[source, bash]
----
[DEFAULT]

transport_url = rabbit://openstack:{{ RABBIT_PASS }}@{{ nodes.controller.name }} <1>

[cors]

[cors.subdomain]

[database]

sqlite_db = /var/lib/glance/glance.sqlite

backend = sqlalchemy

connection = mysql+pymysql://glance:{{ glance_dbpass }}@{{ nodes.controller.name }}/glance <2>

[glance_store]

stores = file,http
default_store = file
filesystem_store_datadir = {{ glance_image_dir }} <3>

[image_format]

disk_formats = ami,ari,aki,vhd,vhdx,vmdk,raw,qcow2,vdi,iso,ploop.root-tar

[keystone_authtoken]

auth_uri = http://{{ nodes.controller.name }}:5000 <4>
auth_url = http://{{ nodes.controller.name }}:35357 <5>
memcached_servers = {{ nodes.controller.name }}:11211 <6>
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = {{ glance_dbpass }} <7>

[matchmaker_redis]

[oslo_concurrency]

[oslo_messaging_amqp]

[oslo_messaging_kafka]

[oslo_messaging_notifications]

driver = messagingv2

[oslo_messaging_rabbit]

[oslo_messaging_zmq]

[oslo_middleware]

[oslo_policy]

[paste_deploy]

flavor = keystone

[profiler]

[store_type_location_strategy]

[task]

[taskflow_executor]
----
<1> Contraseña de RabbitMQ y nombre del nodo de control
<2> Contraseña de Glance y nombre del nodo de control
<3> Directorio donde se vayan a almacenar las imágenes (p.e. `/var/lib/glance/images`)
<4> Nombre del nodo de control
<5> Nombre del nodo de control
<6> Nombre del nodo de control
<7> Contraseña de Glance

****

. Modificar el archivo `/etc/glance/glance-registry.conf`

.El archivo `/etc/glance/glance-registry.conf`
****
[source, bash]
----
[DEFAULT]

transport_url = rabbit://openstack:{{ RABBIT_PASS }}@{{ nodes.controller.name }} <1>

[database]

sqlite_db = /var/lib/glance/glance.sqlite

backend = sqlalchemy

connection = mysql+pymysql://glance:{{ glance_dbpass }}@{{ nodes.controller.name }}/glance <2>

[keystone_authtoken]

auth_uri = http://{{ nodes.controller.name }}:5000 <3>
auth_url = http://{{ nodes.controller.name }}:35357 <4>
memcached_servers = {{ nodes.controller.name }}:11211 <5>
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = {{ glance_dbpass }}

[matchmaker_redis]

[oslo_messaging_amqp]

[oslo_messaging_kafka]

[oslo_messaging_notifications]

driver = messagingv2

[oslo_messaging_rabbit]

[oslo_messaging_zmq]

[oslo_policy]

[paste_deploy]

flavor = keystone

[profiler]
----
<1> Contraseña de RabbitMQ y nombre del nodo de control
<2> Contraseña de Glance y nombre del nodo de control
<3> Nombre del nodo de control
<4> Nombre del nodo de control
<5> Nombre del nodo de control
<6> Contraseña de Glance
****